\section{Methodology}
\label{chap:methodology}

As our goal is experimental reproducibility of results, it behooves us to carefully (and minimally) document the setup required to train and evaluate each permutation of dataset / algorithm that we consider. All our code is and method can be examined in detail on \href{https://github.com/danielmohansahu/benchmarking-3d-semantic-segmentation}{GitHub}.

\subsection{Dependency Handling}
\label{dependency-handling}

We leverage containerization (via \href{https://www.docker.com/}{Docker}) to create isolated build and training environments. Each segmenter has its own Docker image with specific dependency versions installed. We include the source code of the primary implementation of each segmenter directly as a git submodule. In cases where source code changes are necessary we capture these as git patch files for full transparency.

The general workflow is:
\begin{enumerate}
  \item Download Datasets: This process varies from dataset to dataset and is currently fully manual.
  \item Build a bespoke Docker image for each segmenter and mount source code directly in a container.
  \item Train via per-segmenter specific training instructions.
  \item Evaluate via per-segmenter specific validation instructions.
\end{enumerate}

Unfortunately, most situations were not so clear cut. For example, Cylinder3D relies on an unavailable package spconv which must be installed from source. We document additional steps required in the \href{https://github.com/danielmohansahu/benchmarking-3d-semantic-segmentation/blob/main/README.md}{README}.

\subsection{Limitations}
\label{limitations}

Some training and validation conditions are not fully controllable. The largest of these is the hardware requirement. All of the algorithms were originally trained on high performance computing hardware - i.e. multi-GPU machines with much more memory available than we had available. Our results were collected on a laptop with GeForce RTX 2080 SUPER Mobile / Max-Q, Intel Core i7-10750H @2.6GHz x 12, 32 GB RAM. This necessitated changing certain hyperparameters. In select cases this should not affect performance (i.e. where there was parallelization). In other cases the parameters will directly have a deleterious effect, as when we reduce the voxel granularity in Cylinder3D.

\subsection{Metrics}
\label{sec:metrics}

\begin{equation}
  \label{eq:miou}
  mIoU = \frac{1}{N_{class}}\sum_{i=1}^{N_{class}} \frac{TP(i)}{TP(i) + FP(i) + FN(i)}
\end{equation}

\begin{equation}
  \label{eq:oa}
  oa = \frac{TP + TN}{TP + FP + TN + FN}
\end{equation}

Dataset competitions usually rank submissions by either mean Intersection Over Union (mIoU, Equation \ref{eq:miou} where $TP,FP,TN,FN$ refer to true positives, false positives, true negatives, false negatives, respectively) or Overall Accuracy (OA, Equation \ref{eq:oa}, same definitions as mIoU). Both of these are pure evaluations of performance.

We found it illustrative to capture many more metrics than just mIOU and OA, including some pertaining to training performance and data input, as different algorithms are trying to tackle complexities of the training and labeling process (not just overall performance).

\begin{enumerate}
  \item Runtime Rate (hz): This is the max achieved throughput of the model at runtime, i.e. how many frames it can process per second.
  \item Runtime GPU usage - dictates the sort of hardware required to run a trained model.
  \item Time to Train: total amount of time it took to train a model against a dataset.
\end{enumerate}


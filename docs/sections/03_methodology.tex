\section{Methodology}
\label{chap:methodology}

As our goal is experimental reproducibility of results, it behooves us to carefully (and minimally) document the setup required to train and evaluate each permutation of dataset / algorithm that we consider. All our code is and method can be examine in detail on \href{https://github.com/danielmohansahu/benchmarking-3d-semantic-segmentation}{GitHub}.

\subsection{Dependency Handling}
\label{dependency-handling}

We leverage containerization (via \href{https://www.docker.com/}{Docker}) to create isolated build and training environments and freeze dependencies wherever possible. Each segmenter has its own associated Docker image with, e.g., a specific version of Torch installed. We include the source code of the primary implementation of each segmenter directly as a git submodule pointing to the latest relevant release. In cases where source code changes are necessary we capture these as git patch files for full transparency (rather than maintaining a fork).

The general workflow is:
\begin{enumerate}
  \item Download Datasets: This process varies from dataset to dataset, but generally involves making an account to get access. There is not yet a standard for dataset format, so each Segmenter will require a distinct parser for each dataset.
  \item Build a bespoke Docker image for each segmenter and instantiate a Docker Container representing an isolated build environment.
  \item Train via per-segmenter specific training instructions.
  \item Evaluate via per-segmenter specific validation instructions.
\end{enumerate}

Unfortunately, most situations were not so clear cut. For example, Cylinder3D relies on an unavailable package spconv which must be installed from source. We document additional steps required in the \href{https://github.com/danielmohansahu/benchmarking-3d-semantic-segmentation/blob/main/README.md}{README}.

\subsection{Limitations}
\label{limitations}

Some training and validation conditions are not fully controllable. The largest of these is the hardware requirement. Most of the algorithms considered in this study were trained on high performance computing hardware - i.e. multi-GPU machines with much more memory available than we had available. Our results were collected on a laptop with GeForce RTX 2080 SUPER Mobile / Max-Q, Intel Core i7-10750H @2.6GHz x 12, 32 GB RAM. This necessitated changing some training hyperparameters. In select cases this should not affect performance (i.e. where there was parallelization). In other cases the parameters will directly have a deleterious effect, as when we reduce the voxel granularity in Cylinder3D.

One wonders how benchmarking reults would change if each submission was trained and evaluated on the same hardware, and how freezing certain hyperparameters (like number of epochs) would affect the standings.

\subsection{Metrics}
\label{sec:metrics}

\begin{equation}
  \label{eq:miou}
  mIoU = \frac{1}{N_{class}}\sum_{i=1}^{N_{class}} \frac{TP(i)}{TP(i) + FP(i) + FN(i)}
\end{equation}

\begin{equation}
  \label{eq:oa}
  oa = \frac{TP + TN}{TP + FP + TN + FN}
\end{equation}

Dataset competitions usually rank submissions by either mean Intersection Over Union (mIoU, Equation \ref{eq:miou} where $TP,FP,TN,FN$ refer to true positives, false positives, true negatives, false negatives, respectively) or Overall Accuracy (OA, Equation \ref{eq:oa}, same definitions as mIoU). Both of these are pure evaluations of performance.

In addition to these we consider the other following metrics to evaluate aspects of training relevant to a potential user of each algorithm. Each of these metric was collected using the same hardware (GeForce RTX 2080 SUPER Mobile / Max-Q, Intel Core i7-10750H @2.6GHz x 12, 32 GB RAM).

\begin{enumerate}
  \item Runtime Rate (hz): This is the max achieved throughput of the model at runtime, i.e. how many frames it can process per second.
  \item Runtime GPU usage - dictates the sort of hardware required to run a trained model.
  \item Time to Train: total amount of time it took to train a model against a dataset. In some cases this is irrelevant, but given the scale of this metric (> 1 week was not uncommon) it is an important factor to consider for a real world deployment.
\end{enumerate}


\section{Methodology}
\label{chap:methodology}

As our goal is experimental reproducibility of results, it behooves us to carefully (and minimally) document the setup required to train and evaluate each permutation of dataset / algorithm that we consider. All our code is and method can be examine in detail on \href{https://github.com/danielmohansahu/benchmarking-3d-semantic-segmentation}{GitHub}.

\subsection{Dependency Handling}
\label{dependency-handling}

We leverage containerization (via Docker) to create isolated build and training environments and freeze dependencies wherever possible. Each segmenter has its own associated Docker image with, e.g., a specific version of Torch installed. We include the source code of the primary implementation of each segmenter directly as a git submodule pointing to the latest relevant release. In cases where source code changes are necessary we capture these as git patch files for full transparency (rather than maintaining a fork).

The general workflow is:
\begin{enumerate}
  \item Download datasets.
  \item Build segmenter Docker image and enter an interactive container.
  \item Train.
\end{enumerate}

Unfortunately, most situations were not so clear cut. For example, Cylinder3D relies on an unavailable package spconv which must be installed from source. We document additional steps required in the \href{https://github.com/danielmohansahu/benchmarking-3d-semantic-segmentation/blob/main/README.md}{README}.

\subsection{Limitations}
\label{limitations}

Some training and validation conditions are not fully controllable. The largest of these is the hardware requirement. Most of the algorithms considered in this study were trained on high performance computing hardware - i.e. multi-GPU machines with much more memory available than we had available. Our results were collected on a laptop with GeForce RTX 2080 SUPER Mobile / Max-Q, Intel Core i7-10750H @2.6GHz x 12, 32 GB RAM. This necessitated changing some training hyperparameters. In select cases this should not affect performance (i.e. where there was parallelization). In other cases the parameters will directly have a deleterious effect, as when we reduce the voxel granularity in Cylinder3D.

One wonders how benchmarking reults would change if each submission was trained and evaluated on the same hardware, and how freezing certain hyperparameters (like number of epochs) would affect the standings.

\subsection{Metrics}
\label{metrics}

\begin{equation}
  \label{eq:miou}
  mIoU = \frac{1}{N_{class}}\sum_{i=1}^{N_{class}} \frac{TP(i)}{TP(i) + FP(i) + FN(i)}
\end{equation}

\begin{equation}
  \label{eq:oa}
  oa = \frac{TP + TN}{TP + FP + TN + FN}
\end{equation}

Dataset competitions usually rank submissions by either mean Intersection Over Union (mIoU, Equation \ref{eq:miou} where $TP,FP,TN,FN$ refer to true positives, false positives, true negatives, false negatives, respectively) or Overall Accuracy (OA, Equation \ref{eq:oa}, same definitions as mIoU). Both of these are pure evaluations of performance.

In addition to these we consider the other following metrics to evaluate aspects of training relevant to a potential user of each algorithm. Each of these metric was collected using the same hardware (GeForce RTX 2080 SUPER Mobile / Max-Q, Intel Core i7-10750H @2.6GHz x 12, 32 GB RAM).

\begin{enumerate}
  \item Time to Evaluate: total amount of time it took to evaluate a trained model against a dataset. This captures (essentially) runtime performance.
  \item Runtime GPU usage - dictates the sort of hardware required to run a trained model.
  \item Time to Train: total amount of time it took to train a model against a dataset.
\end{enumerate}


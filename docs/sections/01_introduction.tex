\section{Introduction}
\label{chap:introduction}

The potential of autonomous driving has been discussed enough to warrant bypassing here - suffice it to say that the field is extremely important. 3D Semantic Segmentation is a key enabling technology for autonomous driving. Semantic Segmentation (discussed in more detail in Chapter \ref{chap:background}) assigns a semantic label (e.g. "car") to each element of a data stream. Impressive advances have been made in 2D, e.g. where the data stream is an image and the labels are assigned to individual pixels, with the technology advancing enough to be used \href{https://nanonets.com/blog/semantic-image-segmentation-2020/}{outside academia}. The 3D case, however, has certain fundamental challenges which make it exponentially harder.

Recognizing this shortcoming, autonomous vehicle companies have taken to collecting and releasing curated (labeled) datasets with the goal of enabling research into supervised learning techniques. To further spur competition, these datasets generally have an associated challenge (\href{https://waymo.com/open/challenges/}{waymo}, \href{https://codalab.lisn.upsaclay.fr/competitions/6280}{SemanticKitti}, etc.) which rank submissions based on their performance against a reserved test set. This approach has acted as a forcing function for research into 3D Semantic Segmentation methods, with competitors often \href{https://github.com/xinge008/Cylinder3D}{advertising their success} as a badge of honor.

Unfortunately, there is a gap between success in the competition and success in practice. Thee implementation of successful algorithms is often brittle - hardcoding for certain datasets, dependencies on deprecated libraries, etc, all of which make experimental reproducibility a challenge. We propose that Systemic changes to challenge formats would ameliorate some of these problems.

\subsection{Goal}
\label{sec:goal}

Our goal is to train, evaluate, and measure as many permutations of dataset and methodology from Table \ref{tab:datasets} and Table \ref{tab:segmenters} as time allows. We then provide detailed analysis of how these results compare to publicly available benchmarks (where applicable). We also analyze those permutations which cannot be easily / reliably reproduced, distinguishing between resource limitation failures and a genuine technical lack of reproducibility. FInally, we conclude with specific recommendations to improve the evaluation process.

\begin{table}[h!]
\begin{center}
\begin{tabular}{ |c|c|c| }
 \hline
  SemanticKitti \cite{semantickitti1,semantickitti2} & nuScenes \cite {nuscenes} & Rellis3D \cite{rellis3d} \\
 \hline
\end{tabular}
\caption{Publicly available 3D labeled datasets.}
\label{tab:datasets}
\end{center}
\end{table}

\begin{table}[h!]
\begin{center}
\begin{tabular}{ |c|c|c|c| }
 \hline
  Cylinder3D \cite{cylinder3d} & SalsaNext \cite{salsanext} & 2DPASS \cite{2dpass} & COARSE3D \cite{coarse3d} \\
 \hline
\end{tabular}
\end{center}
\caption{3D Semantic Segmentation algorithms}
\label{tab:segmenters}
\end{table}

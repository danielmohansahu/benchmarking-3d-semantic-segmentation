\section{Introduction}
\label{chap:introduction}

The potential of autonomous driving has been discussed enough, and with far more eloquence than I can match, to warrant bypassing that discussion in this paper. Suffice it to say that 3D Semantic Segmentation is a key enabling technology for autonomous vehicles (among other things), which speaks to its importance. Semantic Segmentation (discussed in more detail in Chapter \ref{chap:background}) is a method of assigning a semantic label (e.g. "car", "bush", "human") to each element of a data stream. Impressive advances have been made in the 2D case, e.g. where the data stream is an image and the labels are assigned to individual pixels, with the technology advancing enough to be used \href{https://nanonets.com/blog/semantic-image-segmentation-2020/}{outside academia}. The 3D case, however, has certain fundamental challenges like sparsity which make it an exponentially harder problem to solve.

Recognizing this shortcoming, autonomous vehicle companies have taken to collecting and publicly releasing curated (labeled) datasets with the goal of enabling research into supervised learning techniques. To further spur competition, these datasets generally have an associated challenge (\href{https://waymo.com/open/challenges/}{waymo}, \href{https://codalab.lisn.upsaclay.fr/competitions/6280}{SemanticKitti}, ...) which ranks submissions based on their performance against a reserved test set. This approach has acted as a forcing function for research into 3D Semantic Segmentation methods, with competitors often \href{https://github.com/xinge008/Cylinder3D}{advertising their success} as a badge of honor.

Unfortunately, there is a technological gap here between success in the competition and success "in the real world". When one digs into the implementation of these successful algorithms one often finds fairly brittle code - hardcoding for certain datasets, dependencies on deprecated libraries, etc, all of which make experimental reproducibility a challenge. This isn't to say researchers should shoulder the considerable burden and engineering effort required to turn research code into production ready code, but it doesn't bode well if a top-ranking algorithm can't be reproduced or trivially extended to an alternative dataset.

\subsection{Goal}
\label{sec:goal}

In short, the goal of this project is to train, evaluate, and measure each permutation of dataset and methodology from Table \ref{tab:datasets} and Table \ref{tab:segmenters}. We provide detailed analysis of how these results compare to publicly available benchmarks (where applicable). In addition, we analyze those permutations which cannot be easily / reliably reproduced, distinguishing between resource limitation failures and a genuine technical lack of reproducibility.

\begin{table}[h!]
\begin{center}
\begin{tabular}{ |c|c|c| }
 \hline
  SemanticKitti \cite{semantickitti1} \cite{semantickitti2} & nuScenes \cite {nuscenes} & Rellis3D \cite{rellis3d} \\
 \hline
\end{tabular}
\caption{Publicly available 3D labeled datasets.}
\label{tab:datasets}
\end{center}
\end{table}

\begin{table}[h!]
\begin{center}
\begin{tabular}{ |c|c|c|c| }
 \hline
  Cylinder3D \cite{cylinder3d} & SalsaNext \cite{salsanext} & 2DPASS \cite{2dpass} & COARSE3D \cite{coarse3d} \\
 \hline
\end{tabular}
\end{center}
\caption{3D Semantic Segmentation algorithms}
\label{tab:segmenters}
\end{table}
